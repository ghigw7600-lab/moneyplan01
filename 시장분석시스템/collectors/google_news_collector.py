# -*- coding: utf-8 -*-
"""
Google News RSS Collector
ë„¤ì´ë²„ ë‰´ìŠ¤ ì™¸ì— Google Newsì—ì„œë„ ë‰´ìŠ¤ ìˆ˜ì§‘
"""

import requests
import xml.etree.ElementTree as ET
from datetime import datetime
import time


class GoogleNewsCollector:
    """Google News RSS ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.base_url = "https://news.google.com/rss/search"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def get_news(self, keyword, max_count=20, language='ko'):
        """
        Google Newsì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰

        Args:
            keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count (int): ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜
            language (str): ì–¸ì–´ ('ko' ë˜ëŠ” 'en')

        Returns:
            list: ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\nğŸ“° Google News ê²€ìƒ‰: '{keyword}' (ìµœëŒ€ {max_count}ê°œ)")

        try:
            # RSS URL êµ¬ì„±
            params = {
                'q': keyword,
                'hl': language,  # ì–¸ì–´
                'gl': 'KR',      # êµ­ê°€
                'ceid': 'KR:ko'  # ì§€ì—­:ì–¸ì–´
            }

            # URL ì¸ì½”ë”©
            query = f"?q={requests.utils.quote(keyword)}&hl={language}&gl=KR&ceid=KR:{language}"
            url = self.base_url + query

            print(f"ğŸ” URL: {url}")

            # RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸°
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()

            # XML íŒŒì‹±
            root = ET.fromstring(response.content)

            news_list = []

            # RSS ì•„ì´í…œ ì¶”ì¶œ
            for item in root.findall('.//item')[:max_count]:
                try:
                    title = item.find('title').text if item.find('title') is not None else ''
                    link = item.find('link').text if item.find('link') is not None else ''
                    pub_date = item.find('pubDate').text if item.find('pubDate') is not None else ''
                    description = item.find('description').text if item.find('description') is not None else ''
                    source = item.find('source').text if item.find('source') is not None else 'Google News'

                    # ë‚ ì§œ íŒŒì‹± (RFC 822 í˜•ì‹)
                    try:
                        dt = datetime.strptime(pub_date, '%a, %d %b %Y %H:%M:%S %Z')
                        formatted_date = dt
                    except:
                        formatted_date = datetime.now()

                    # êµ­ë‚´/ì™¸ì‹  íŒë‹¨ (URL ë„ë©”ì¸ ê¸°ë°˜ + ì–¸ë¡ ì‚¬ëª… í™•ì¥)
                    # 1. URL ë„ë©”ì¸ ì²´í¬ (.kr, .co.kr)
                    is_domestic = False
                    if link:
                        if '.kr' in link.lower() or 'naver.com' in link.lower() or 'daum.net' in link.lower():
                            is_domestic = True

                    # 2. í•œêµ­ ì–¸ë¡ ì‚¬ DB (100+ í™•ì¥)
                    if not is_domestic:
                        domestic_sources = [
                            # ì£¼ìš” ì¢…í•©ì§€
                            'ì—°í•©ë‰´ìŠ¤', 'ë‰´ìŠ¤1', 'ë‰´ì‹œìŠ¤', 'ë…¸ì»·ë‰´ìŠ¤', 'ë‰´ìŠ¤í•Œ',

                            # ë°©ì†¡ì‚¬
                            'SBS', 'KBS', 'MBC', 'JTBC', 'YTN', 'TVì¡°ì„ ', 'ì±„ë„A', 'MBN',

                            # ì¤‘ì•™ ì¼ê°„ì§€
                            'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´', 'í•œê²¨ë ˆ', 'ê²½í–¥ì‹ ë¬¸', 'êµ­ë¯¼ì¼ë³´', 'ì„œìš¸ì‹ ë¬¸', 'ë¬¸í™”ì¼ë³´', 'ì„¸ê³„ì¼ë³´',

                            # ê²½ì œì§€
                            'ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ê²½ì œ', 'í—¤ëŸ´ë“œê²½ì œ', 'ì•„ì‹œì•„ê²½ì œ', 'íŒŒì´ë‚¸ì…œë‰´ìŠ¤', 'ì´ë°ì¼ë¦¬', 'ë¨¸ë‹ˆíˆ¬ë°ì´', 'ë¹„ì¦ˆë‹ˆìŠ¤ì›Œì¹˜', 'ë¨¸ë‹ˆS', 'ë‰´ìŠ¤í† ë§ˆí† ', 'ë°ì¼ë¦¬ì•ˆ', 'ì¿ í‚¤ë‰´ìŠ¤',

                            # IT/í…Œí¬
                            'ë””ì§€í„¸íƒ€ì„ìŠ¤', 'ì „ìì‹ ë¬¸', 'ì§€ë””ë„·ì½”ë¦¬ì•„', 'ë¸”ë¡œí„°', 'í…Œí¬M', 'ì•„ì´ë‰´ìŠ¤24', 'ë””ì§€í„¸ë°ì¼ë¦¬',

                            # ì§€ì—­ì§€
                            'ë¶€ì‚°ì¼ë³´', 'êµ­ì œì‹ ë¬¸', 'ì˜ë‚¨ì¼ë³´', 'ëŒ€êµ¬ì‹ ë¬¸', 'ê´‘ì£¼ì¼ë³´', 'ì „ë¶ì¼ë³´', 'ê°•ì›ì¼ë³´', 'ì œì£¼ì¼ë³´', 'ì¶©ì²­ì¼ë³´', 'ëŒ€ì „ì¼ë³´', 'ê²½ê¸°ì¼ë³´', 'ì¸ì²œì¼ë³´',

                            # ì „ë¬¸ì§€
                            'ì˜í•™ì‹ ë¬¸', 'ë©”ë””ì»¬íƒ€ì„ì¦ˆ', 'ì²­ë…„ì˜ì‚¬', 'ì•½ì‚¬ê³µë¡ ', 'ë³´í—˜ì‹ ë¬¸', 'ì „ê¸°ì‹ ë¬¸', 'ê°€ìŠ¤ì‹ ë¬¸', 'ì—ë„ˆì§€ì‹ ë¬¸', 'ê±´ì„¤ê²½ì œ', 'í™˜ê²½ì¼ë³´',

                            # ì¸í„°ë„· ë§¤ì²´
                            'ì˜¤ë§ˆì´ë‰´ìŠ¤', 'í”„ë ˆì‹œì•ˆ', 'ë¯¸ë””ì–´ì˜¤ëŠ˜', 'ì‹œì‚¬IN', 'ì£¼ê°„ê²½í–¥', 'í•œê²¨ë ˆ21', 'ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸', 'ì¡°ì„ ë¹„ì¦ˆ', 'ì¤‘ì•™ì¼ë³´', 'ë§¤ê²½ì´ì½”ë…¸ë¯¸', 'ì‹œì‚¬ì €ë„',

                            # ìŠ¤í¬ì¸ /ì—°ì˜ˆ
                            'ìŠ¤í¬ì¸ ì¡°ì„ ', 'ìŠ¤í¬ì¸ ë™ì•„', 'ìŠ¤í¬ì¸ ì„œìš¸', 'ì¼ê°„ìŠ¤í¬ì¸ ', 'OSEN', 'ìŠ¤íƒ€ë‰´ìŠ¤', 'ì—‘ìŠ¤í¬ì¸ ë‰´ìŠ¤', 'í…ì•„ì‹œì•„', 'ë§ˆì´ë°ì¼ë¦¬', 'ìŠ¤í¬í‹°ë¹„ë‰´ìŠ¤',

                            # ê¸°íƒ€
                            'ì½”ë¦¬ì•„í—¤ëŸ´ë“œ', 'ì½”ë¦¬ì•„íƒ€ì„ìŠ¤', 'ë¯¼ì¤‘ì˜ì†Œë¦¬', 'ì°¸ì„¸ìƒ', 'ë ˆë””ì•™', 'ë°ì¼ë¦¬NK', 'ììœ ì•„ì‹œì•„ë°©ì†¡', 'ì¤‘ì•™ì„ ë°ì´', 'í•œêµ­ì¼ë³´', 'ì¡°ì„¸ì¼ë³´'
                        ]
                        is_domestic = any(ds in source for ds in domestic_sources)

                    news_list.append({
                        'ì œëª©': title,
                        'ë§í¬': link,
                        'ë‚ ì§œ': formatted_date,
                        'ì„¤ëª…': description,
                        'ì–¸ë¡ ì‚¬': source,
                        'ì¶œì²˜': 'Google News',
                        'source_type': 'domestic' if is_domestic else 'foreign'
                    })

                except Exception as e:
                    print(f"   âš ï¸ ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue

            print(f"âœ… Google News {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list

        except requests.exceptions.Timeout:
            print(f"â° Google News íƒ€ì„ì•„ì›ƒ")
            return []
        except requests.exceptions.RequestException as e:
            print(f"âŒ Google News API ì˜¤ë¥˜: {str(e)}")
            return []
        except ET.ParseError as e:
            print(f"âŒ XML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return []
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
            return []

    def get_finance_news(self, keyword, max_count=20):
        """
        ê¸ˆìœµ/ê²½ì œ ë‰´ìŠ¤ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë³´ê°•)

        Args:
            keyword (str): ê¸°ë³¸ í‚¤ì›Œë“œ
            max_count (int): ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜

        Returns:
            list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
        finance_keywords = [
            f"{keyword} ì£¼ê°€",
            f"{keyword} ì‹¤ì ",
            f"{keyword} ì „ë§",
            f"{keyword} íˆ¬ì"
        ]

        all_news = []
        seen_titles = set()  # ì¤‘ë³µ ì œê±°

        for finance_keyword in finance_keywords:
            news_list = self.get_news(finance_keyword, max_count=max_count // len(finance_keywords))

            # ì¤‘ë³µ ì œê±°
            for news in news_list:
                if news['ì œëª©'] not in seen_titles:
                    seen_titles.add(news['ì œëª©'])
                    all_news.append(news)

            time.sleep(0.5)  # Rate limiting

        print(f"\nâœ… ì´ {len(all_news)}ê°œ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ ì œê±°)")
        return all_news

    def get_multi_source_news(self, keyword, max_count=30):
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘

        Args:
            keyword (str): ê²€ìƒ‰ í‚¤ì›Œë“œ
            max_count (int): ìµœëŒ€ ë‰´ìŠ¤ ê°œìˆ˜

        Returns:
            list: í†µí•© ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“¡ ë‹¤ì¤‘ ì†ŒìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘: '{keyword}'")
        print(f"{'='*60}\n")

        all_news = []

        # Google News
        google_news = self.get_news(keyword, max_count=max_count)
        all_news.extend(google_news)

        print(f"\nâœ… ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_news


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    collector = GoogleNewsCollector()

    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_keywords = ['ì‚¼ì„±ì „ì', 'Bitcoin', 'NVIDIA']

    for keyword in test_keywords:
        news_list = collector.get_news(keyword, max_count=5)

        print(f"\n{'='*60}")
        print(f"ğŸ” '{keyword}' ê²€ìƒ‰ ê²°ê³¼ ({len(news_list)}ê°œ)")
        print(f"{'='*60}\n")

        for i, news in enumerate(news_list, 1):
            print(f"{i}. {news['ì œëª©']}")
            print(f"   ì–¸ë¡ ì‚¬: {news['ì–¸ë¡ ì‚¬']}")
            print(f"   ë‚ ì§œ: {news['ë‚ ì§œ'].strftime('%Y-%m-%d %H:%M')}")
            print(f"   ë§í¬: {news['ë§í¬'][:60]}...")
            print()

        time.sleep(1)  # Rate limiting
