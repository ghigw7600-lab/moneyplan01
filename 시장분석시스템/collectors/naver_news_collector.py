# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ (í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ í¬í•¨)
Phase 4-2: ìºì‹œ ì‹œìŠ¤í…œ ì¶”ê°€ (1ì‹œê°„ ìœ íš¨)
"""
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import re
import json
import os
import hashlib


class NaverNewsCollector:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (Phase 4-2: ìºì‹œ ì§€ì›)"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        # Phase 4-2: ìºì‹œ ì„¤ì •
        self.cache_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'cache', 'news')
        os.makedirs(self.cache_dir, exist_ok=True)
        self.cache_validity = 3600  # 1ì‹œê°„ (3600ì´ˆ)

    def _get_cache_key(self, query, max_count):
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_string = f"{query}_{max_count}"
        return hashlib.md5(key_string.encode()).hexdigest()

    def _load_cache(self, query, max_count):
        """ìºì‹œì—ì„œ ë‰´ìŠ¤ ë¡œë“œ"""
        cache_key = self._get_cache_key(query, max_count)
        cache_file = os.path.join(self.cache_dir, f"naver_{cache_key}.json")

        if not os.path.exists(cache_file):
            return None

        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            # ìºì‹œ ìœ íš¨ì„± í™•ì¸
            cache_time = cache_data.get('timestamp', 0)
            if time.time() - cache_time > self.cache_validity:
                return None

            return cache_data.get('news', [])
        except:
            return None

    def _save_cache(self, query, max_count, news):
        """ìºì‹œì— ë‰´ìŠ¤ ì €ì¥"""
        try:
            cache_key = self._get_cache_key(query, max_count)
            cache_file = os.path.join(self.cache_dir, f"naver_{cache_key}.json")

            cache_data = {
                'timestamp': time.time(),
                'news': news
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except:
            pass

    def get_news(self, query, max_count=20, use_cache=True):
        """
        ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìˆ˜ì§‘ (Phase 4-2: ìºì‹œ ì§€ì›)

        Args:
            query (str): ê²€ìƒ‰ í‚¤ì›Œë“œ (ì˜ˆ: "ì‚¼ì„±ì „ì", "ë¹„íŠ¸ì½”ì¸")
            max_count (int): ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜
            use_cache (bool): ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [{'title', 'description', 'url', 'date', 'source'}]
        """
        # Phase 4-2: ìºì‹œ í™•ì¸
        if use_cache:
            cached_news = self._load_cache(query, max_count)
            if cached_news is not None:
                return cached_news

        news_list = []

        try:
            # ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ URL
            base_url = "https://search.naver.com/search.naver"

            for start in range(1, max_count + 1, 10):
                params = {
                    'where': 'news',
                    'query': query,
                    'start': start,
                    'sort': 1  # ìµœì‹ ìˆœ (0: ê´€ë ¨ë„ìˆœ, 1: ìµœì‹ ìˆœ)
                }

                response = requests.get(base_url, params=params, headers=self.headers, timeout=10)

                if response.status_code != 200:
                    print(f"âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: HTTP {response.status_code}")
                    break

                soup = BeautifulSoup(response.text, 'html.parser')

                # ë‰´ìŠ¤ í•­ëª© íŒŒì‹±
                articles = soup.select('div.news_area')

                if not articles:
                    print(f"âš ï¸ ë” ì´ìƒ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                    break

                for article in articles:
                    try:
                        # ì œëª© ë° ë§í¬
                        title_elem = article.select_one('a.news_tit')
                        if not title_elem:
                            continue

                        title = title_elem.get('title', title_elem.get_text(strip=True))
                        url = title_elem.get('href', '')

                        # ìš”ì•½
                        desc_elem = article.select_one('div.news_dsc')
                        description = desc_elem.get_text(strip=True) if desc_elem else ''

                        # ì–¸ë¡ ì‚¬
                        source_elem = article.select_one('a.info.press')
                        source = source_elem.get_text(strip=True) if source_elem else 'ì•Œ ìˆ˜ ì—†ìŒ'

                        # ë‚ ì§œ
                        date_elem = article.select_one('span.info')
                        date_text = date_elem.get_text(strip=True) if date_elem else ''

                        # ë‚ ì§œ íŒŒì‹±
                        published_date = self._parse_date(date_text)

                        news_list.append({
                            'ì œëª©': title,
                            'ì„¤ëª…': description,
                            'ë§í¬': url,  # í‚¤ í†µì¼
                            'ë‚ ì§œ': published_date,
                            'ì–¸ë¡ ì‚¬': source,
                            'ì¶œì²˜': 'ë„¤ì´ë²„ ë‰´ìŠ¤',
                            'source_type': 'domestic'  # ë„¤ì´ë²„ëŠ” ëª¨ë‘ êµ­ë‚´ ë‰´ìŠ¤
                        })

                        if len(news_list) >= max_count:
                            break

                    except Exception as e:
                        print(f"âš ï¸ ë‰´ìŠ¤ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                        continue

                if len(news_list) >= max_count:
                    break

                # ìš”ì²­ ê°„ ë”œë ˆì´ (ë„¤ì´ë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(0.5)

            print(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")

            # Phase 4-2: ìºì‹œ ì €ì¥
            if use_cache:
                self._save_cache(query, max_count, news_list)

            return news_list

        except Exception as e:
            print(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
            return news_list

    def _parse_date(self, date_text):
        """ë‚ ì§œ í…ìŠ¤íŠ¸ íŒŒì‹±"""
        try:
            now = datetime.now()

            # "Në¶„ ì „", "Nì‹œê°„ ì „", "Nì¼ ì „" í˜•ì‹
            if 'ë¶„ ì „' in date_text:
                minutes = int(re.search(r'(\d+)ë¶„', date_text).group(1))
                return now - timedelta(minutes=minutes)
            elif 'ì‹œê°„ ì „' in date_text:
                hours = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
                return now - timedelta(hours=hours)
            elif 'ì¼ ì „' in date_text:
                days = int(re.search(r'(\d+)ì¼', date_text).group(1))
                return now - timedelta(days=days)
            else:
                # "YYYY.MM.DD" í˜•ì‹
                try:
                    return datetime.strptime(date_text, '%Y.%m.%d')
                except:
                    return now

        except:
            return datetime.now()

    def get_finance_news(self, ticker, stock_name, max_count=20):
        """
        íŠ¹ì • ì¢…ëª© ê´€ë ¨ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘

        Args:
            ticker (str): ì¢…ëª© ì½”ë“œ (ì˜ˆ: "005930")
            stock_name (str): ì¢…ëª©ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
            max_count (int): ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜

        Returns:
            list: ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        # ì¢…ëª©ëª…ìœ¼ë¡œ ê²€ìƒ‰ (ë” ì •í™•í•œ ê²°ê³¼)
        return self.get_news(stock_name, max_count)


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    collector = NaverNewsCollector()

    print("=== ì‚¼ì„±ì „ì ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ===")
    news = collector.get_news("ì‚¼ì„±ì „ì", max_count=10)

    for i, article in enumerate(news, 1):
        print(f"\n[{i}] {article['ì œëª©']}")
        print(f"    ğŸ“° {article['ì–¸ë¡ ì‚¬']} | {article['ë‚ ì§œ'].strftime('%Y-%m-%d %H:%M')}")
        print(f"    ğŸ”— {article['url']}")
        print(f"    ğŸ“ {article['ì„¤ëª…'][:100]}...")

    print(f"\nì´ {len(news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
